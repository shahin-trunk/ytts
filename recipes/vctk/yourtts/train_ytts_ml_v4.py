import json
import os

import torch
from trainer import Trainer, TrainerArgs

from TTS.config.shared_configs import BaseDatasetConfig
from TTS.tts.configs.vits_config import VitsConfig
from TTS.tts.datasets import load_tts_samples
from TTS.tts.models.vits import CharactersConfig, Vits, VitsArgs, VitsAudioConfig
from TTS.tts.utils.text.characters import _pad, _blank, _bos, _eos

torch.set_num_threads(24)

CHARACTERS = "".join(sorted(
    {' ', 'a', 'b', 'c', 'd', 'e', 'f', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',
     'x', 'z', 'Ã¦', 'Ã§', 'Ã°', 'Ä§', 'Å‹', 'É', 'É‘', 'É’', 'É”', 'É•', 'É–', 'É™', 'Éš', 'É›', 'Éœ', 'ÉŸ', 'É¡', 'É£', 'É¨', 'Éª', 'É¬',
     'É­', 'É²', 'É³', 'É¹', 'É¾', 'Ê‚', 'Êƒ', 'Êˆ', 'ÊŠ', 'Ê‹', 'ÊŒ', 'Ê’', 'Ê”', 'Ê•', 'Ê°', 'Ê²', 'Ëˆ', 'ËŒ', 'Ë', 'Ë¤', 'Ìƒ', 'Ì©', 'Ìª',
     'Î¸', 'Ï‡', 'áµ»'}))
PUNCTUATIONS = "".join(sorted({'!', '"', "'", ',', '-', '.', ':', ';', '?', 'ØŒ', 'Ø›', 'ØŸ'}))

RUN_NAME = "YTTS-AR-IIAI"
EXP_ID = "v4_ML"
REF_EXP_ID = "v3_ML"
SPK_EMBEDDING_VERSION = "v1"
PHN_CACHE_VERSION = "v1"
LNG_EMBEDDING_VERSION = "v1"
BASE_PATH = "/data/asr/workspace/audio/tts"
DATA_PATH = "data/audio/multi_lang"
EXPMT_PATH = os.path.join(BASE_PATH, f"expmt/ytts/{EXP_ID}")
REF_EXPMT_PATH = os.path.join(BASE_PATH, f"expmt/ytts/{REF_EXP_ID}")
PHN_CACHE_PATH = os.path.join(REF_EXPMT_PATH, f"phn_cache_{PHN_CACHE_VERSION}")
SPK_EMB_CACHE_PATH = os.path.join(REF_EXPMT_PATH, f"spk_emb_{SPK_EMBEDDING_VERSION}")
LNG_EMB_CACHE_PATH = os.path.join(REF_EXPMT_PATH, f"lng_emb_{LNG_EMBEDDING_VERSION}")
RESTORE_PATH = os.path.join(BASE_PATH,
                            "expmt/ytts/v1_ML/YTTS-AR-IIAI-August-23-2023_03+34PM-452d4855/best_model.pth")

os.makedirs(PHN_CACHE_PATH, exist_ok=True)
# os.makedirs(SPK_EMB_CACHE_PATH, exist_ok=True)
os.makedirs(LNG_EMB_CACHE_PATH, exist_ok=True)

LNG_EMB = {
    "hi": 0,
    "en": 1,
    "ar": 3,
    "ml": 4,
}

LNG_EMB_FILE = os.path.join(LNG_EMB_CACHE_PATH, "language_ids.json")
with open(LNG_EMB_FILE, mode="w") as lef:
    json.dump(LNG_EMB, lef)

SKIP_TRAIN_EPOCH = False
BATCH_SIZE = 32
EVAL_BATCH_SIZE = 4
SAMPLE_RATE = 22050
MAX_AUDIO_LEN_IN_SECONDS = 16
NUM_RESAMPLE_THREADS = 10


def get_dataset(manifest_train: str, manifest_eval: str, d_name: str, lang: str = "ar"):
    return BaseDatasetConfig(
        formatter="iiai_tts",
        dataset_name=f"{lang}_{d_name}",
        meta_file_train=os.path.join(DATA_PATH, manifest_train),
        meta_file_val=os.path.join(DATA_PATH, manifest_eval),
        path=BASE_PATH,
        language=lang,
    )


DATASETS_CONFIG_LIST = [
    get_dataset(manifest_train="manifest_gen_ar_ns_phoneme_ut_2000.json",
                manifest_eval="manifest_gen_ar_ns_phoneme_eval.json",
                d_name="gen",
                lang="ar"),
    # get_dataset(manifest_train="manifest_org_ar_ns_phoneme.json",
    #             manifest_eval="manifest_org_ar_ns_phoneme_eval.json",
    #             d_name="gen",
    #             lang="ar"),
    # get_dataset(manifest_train="manifest_org_en_ns_phoneme.json",
    #             manifest_eval="manifest_org_en_ns_phoneme_eval.json",
    #             d_name="org",
    #             lang="en"),
    get_dataset(manifest_train="manifest_gen_en_ns_phoneme_ut_1000.json",
                manifest_eval="manifest_gen_en_ns_phoneme_eval.json",
                d_name="gen",
                lang="en"),
    get_dataset(manifest_train="manifest_org_hi_ns_phoneme.json",
                manifest_eval="manifest_org_hi_ns_phoneme_eval.json",
                d_name="org",
                lang="hi"),
    get_dataset(manifest_train="manifest_org_ml_ns_phoneme.json",
                manifest_eval="manifest_org_ml_ns_phoneme_eval.json",
                d_name="org",
                lang="ml"),
]

SPEAKER_ENCODER_CHECKPOINT_PATH = (
    os.path.join(BASE_PATH, "expmt/se/multi/v8/run-August-22-2023_09+48AM-452d4855/checkpoint_137000.pth"))
SPEAKER_ENCODER_CONFIG_PATH = os.path.join(BASE_PATH,
                                           "expmt/se/multi/v8/run-August-22-2023_09+48AM-452d4855/config.json")

D_VECTOR_FILES = []

for dataset_conf in DATASETS_CONFIG_LIST:
    d_cache_dir = os.path.join(SPK_EMB_CACHE_PATH, f"{dataset_conf.dataset_name}")
    d_vector_cache_path = os.path.join(d_cache_dir, "d_vector_files.json")
    with open(d_vector_cache_path, encoding="utf-8") as dvcp:
        d_v_files = json.load(dvcp)

    d_v_files = [d_v_file if BASE_PATH in d_v_file else os.path.join(BASE_PATH, d_v_file) for d_v_file in d_v_files]
    D_VECTOR_FILES.extend(d_v_files)

print(f"D_VECTOR_FILES: {D_VECTOR_FILES}")

# Audio config used in training.
audio_config = VitsAudioConfig(
    sample_rate=SAMPLE_RATE,
    hop_length=256,
    win_length=1024,
    fft_size=2048,
    num_mels=400,
)

# Init VITSArgs setting the arguments that are needed for the YourTTS model
model_args = VitsArgs(
    use_sdp=False,
    d_vector_file=D_VECTOR_FILES,
    use_d_vector_file=True,
    d_vector_dim=2048,
    out_channels=1025,
    num_heads_text_encoder=4,
    num_layers_text_encoder=12,
    num_layers_dp_flow=8,
    num_hidden_channels_dp=256,
    num_layers_flow=32,
    num_layers_posterior_encoder=32,
    speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,
    speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,
    use_speaker_encoder_as_loss=True,
    # use_language_embedding=True,
    # embedded_language_dim=8,
)

# General training config, here you can change the batch size and others useful parameters
config = VitsConfig(
    epochs=10000,
    output_path=EXPMT_PATH,
    lr_gen=0.0003,
    lr_disc=0.0003,
    model_args=model_args,
    run_name=RUN_NAME,
    project_name=RUN_NAME,
    run_description=RUN_NAME,
    dashboard_logger="tensorboard",
    audio=audio_config,
    batch_size=BATCH_SIZE,
    batch_group_size=48,
    eval_batch_size=EVAL_BATCH_SIZE,
    num_loader_workers=16,
    print_step=100,
    plot_step=300,
    log_model_step=1000,
    save_step=10000,
    save_n_checkpoints=3,
    save_checkpoints=True,
    print_eval=True,
    # use_phonemes=False,
    # phonemizer="espeak",
    # phoneme_language="ar",
    compute_input_seq_cache=True,
    add_blank=True,
    text_cleaner="ar_bw_cleaners",
    # use_language_embedding=True,
    # language_ids_file=LNG_EMB_FILE,
    characters=CharactersConfig(
        characters_class="TTS.tts.models.vits.VitsCharacters",
        pad=_pad,
        eos=_eos,
        bos=_bos,
        blank=_blank,
        characters=CHARACTERS,
        punctuations=PUNCTUATIONS,
        is_unique=True,
        is_sorted=True,
    ),
    # precompute_num_workers=8,
    # phoneme_cache_path=PHN_CACHE_PATH,
    start_by_longest=True,
    datasets=DATASETS_CONFIG_LIST,
    cudnn_benchmark=False,
    max_audio_len=SAMPLE_RATE * MAX_AUDIO_LEN_IN_SECONDS,
    mixed_precision=False,
    use_d_vector_file=True,
    d_vector_dim=1024,
    d_vector_file=D_VECTOR_FILES,
    test_sentences=[
        [
            'Ê”attËËŒadaÏ‡Ï‡ËˆulaËtËŒi ËˆaÊ•tamËŒadt Ê•ËˆalaË Ê•ËŒamalËˆiËaËtËŒi Ê”asssËˆuËqi Ê”almaftËˆuËÄ§atËŒiØŒ Ê”Ëˆai tËˆilka Ê”almËŒuÊ•aËmËˆalaËtËŒu Ê”ËˆallatËŒiË tËˆasmaÄ§ËŒu lilbËŒanki Ê”almarkËˆaziËËŒi bitËŒaufËŒiËÉ¹i Ê”Ëˆau astËˆinzaËfËŒi Ê”asssËŒuiËˆuËlatËŒi qasÌªËˆi.ËÉ¹atËŒi Ê”alÊ”ËˆadÊ’alËŒi muqËˆaËbilËŒa Ê”aËˆuÉ¹aËqËŒin maËlËˆiËatËŒin mËˆin Ê”almuqÉ¹ËˆidË¤i.ËnËŒaØŒ Ä§ËˆasbamËŒaË qËˆaËla Ê”ËˆaÊƒÏ‡aËsÌªËŒun mËŒutÌªËa.lËˆiÊ•uËnËŒa Ê•ËˆalaË Ê”alÊ”Ëˆamr.',
            "ar_spk_1",
            None,
            "ar",
        ],
        [
            'wËˆaÊ”aÄ§sÌªËŒa.t Ä§ukËˆuËmatËŒu Ê”alwilËˆaËjatËŒi mÊ”tËˆaËn wËŒaÎ¸amaËnËˆiËatËŒa waÊ•ËˆiÊƒÉ¹uËnËŒa ÊƒËˆaÏ‡sÌªaË Ê”aËÏ‡ËˆaÉ¹iËnËŒa Ê”ËˆasÌªbaÄ§ËŒuË bilËŒaË mËˆaÊ”waØŒ baËŒinamËŒaË tËˆamma Ê”ËˆidÊ’laËÊ”ËŒu Î¸ËˆalaËÎ¸ËŒi mËˆiÊ”atËŒin wËŒaÎ¸amaËnËˆiËatËŒin wËŒaÎ¸alËˆaËÎ¸uËnËŒa ÊƒËˆaÏ‡sÌªaË mËˆin Ê”almintÌªËˆa.qatËŒi Ê”asssËŒaËÄ§ilËˆiËatËŒi ÊƒËˆamaËlËŒa madËˆiËnatËŒi saËw baËwlËŒuËØŒ fiË Ê”alwËˆaqti Ê”ËˆallaÃ°ËŒiË tËˆaÊ•malËŒu fiËhËŒi tÌªËŒa.u.ËˆaËqimËŒi Ê”alÊ”ËˆinqaËÃ°ËŒi Ê•ËˆalaË mËŒusaËÊ•ËˆadatËŒi Ê”almËŒutadË¤a.rÉ¹ËˆiÉ¹iËnËŒa mËˆin Ê”alÊ•aËsÌªËˆi.fat.',
            "ar_spk_2",
            None,
            "ar",
        ],
        [
            'Ä§ËˆaÃ°Ã°aÉ¹ËŒa É¹ËˆaÊ”iËsËŒu Ê”alwuzËˆaÉ¹aËÊ”ËŒi Ê”assswËˆidiËËŒu Ê”ËˆuËlf kÉ¹iËstrsËŒuËn mËˆin fasÌªlËŒi tÌªËˆa.labËŒi Ê•udË¤wËˆiËatËŒi bilËŒaËdihËŒi fiË Ä§Ëˆilfi ÊƒËˆamaËlËŒi Ê”alÊ”atÌªlËˆasiËËŒi Ê•Ëˆan finlËŒandaËØŒ waÃ°ËˆalikËŒa baÊ•dËŒa aÊ•tËˆiÉ¹aËfËŒi Ê”alÄ§Ëˆilfi liÊ”ËŒauËËŒali mËˆarÉ¹atËŒin biÊ”ËŒannahËŒu qËˆad jËŒataÊ•aËˆiËanËŒu fasÌªlËŒu Ê”atÌªËËŒa.labËˆainËŒi Ê•ËˆalaË Ï‡alfËˆiËatËŒi aÊ•tËˆiÉ¹aËdË¤ËŒi. tËˆurkiËËŒaË.',
            "ar_spk_3",
            None,
            "ar",
        ],
        [
            'qËˆaËla Ê”arrÉ¹ËˆaÊ”iËsËŒu Ê”attËanfËˆiËÃ°iËËŒu liÊ”ËŒaÄ§adËŒi Ê”ËˆakbaÉ¹ËŒi Ê”albunËŒuËki Ê”alÊ•aËmËˆilatËŒi fiË Ê”asssËŒuÊ•uËdËˆiËatËŒi Ê”Ëˆinna dÊ’ËˆuhuËdËŒa Ê”almamlËˆakatËŒi litËŒaÏ‡fiËfËŒi Ê”ËˆazmatËŒi Ê”asssËŒuiËˆuËlatËŒi Ê”alÊ”aÏ‡ËˆiËÉ¹atËŒi Ê”ËˆallatËŒiË ÊƒahËˆidahËŒaË Ê”annnËˆiÃ°a.ËmËŒu Ê”almËˆaËliËËŒu badËŒaÊ”at tËˆuÊ”tiË Î¸imËˆaËÉ¹ahËŒaË watËˆaÊ•malËŒu Ê•ËˆalaË tahdËˆiÊ”atËŒi Ê”alÊ”ËˆaswaËq.',
            "ar_spk_4",
            None,
            "ar",
        ],
        [
            'jËˆaÊ•malËŒu Ê”almËˆadÊ’lisËŒu Ê•ËˆalaË fadË¤dË¤ËŒi. Ê”almËŒunaËzËˆaÊ•aËtËŒi wËŒataswiËËˆatihËŒaË wËŒaÊ”idaËÉ¹ËˆatihËŒaË dËˆaËÏ‡ilËŒa alËŒiaËtËËˆiÄ§aËdËŒi Ê”alÊ”ifÉ¹ËˆiËqiËËŒiØŒ wËŒaiusËˆaËÊ•idËŒu fiË Ê”attËËˆaÄ§dË¤i.ËÉ¹ËŒi litËŒanÃ°i.ËmËŒi alËŒiaËntiÏ‡ËˆaËbaËtËŒi walÊ”ËˆiÊƒÉ¹aËfËŒi Ê•alËˆaihËŒaË fiË Ê”addËËˆuËalËŒi Ê”alÊ”ËˆaÊ•dË¤a.ËÊ”ËŒiØŒ wËŒaiËˆahdifËŒu Ê”ËˆilaË tËˆaÊ•ziËzËŒi Ê”asssËˆalaËmËŒi walÊ”Ëˆamni fiË Ê”ifÉ¹ËˆiËqiËŒaË.',
            "ar_spk_5",
            None,
            "ar",
        ],
        [
            'Ê”ËˆatÌªlaqt kuËrjËŒaË Ê”aÊƒÊƒËŒamaËlËˆiËatËŒiØŒ Ê”aljËˆaumËŒa sÌªËˆa.ËÉ¹uËÏ‡ËŒaË baËlËŒistiËËŒaË É£ËˆaiÉ¹ËŒa muÄ§ËˆadËadËŒin biËŒaËtËidÊ’ËŒaËhi baÄ§É¹ËŒi Ê”aljËˆaËbaËnËŒiØŒ hËˆuËa Ê”aÎ¸Î¸Î¸ËˆaËniË fiË Ê”ËˆaqallËŒa mËˆin Î¸ËŒamaËnËˆiËatËŒi wËŒaÊ”arbËˆaÊ•uËnËŒa saËÊ•ËŒatanØŒ waËˆiaÊ”tËŒiË É£ËˆadaËtËŒa tadÉ¹ËˆiËbaËtËŒin mËŒuÊƒtaÉ¹ËˆakatËŒin baËŒina dÊ’aËˆiÊƒaËŒi Ê”alwilËˆaËjaËtËŒi Ê”almËŒutËaÄ§ËˆidatËŒi wËˆakuËrjËŒaË Ê”aldÊ’ËŒanuËbËˆiËatËŒiØŒ wËˆafqa mËˆaË Ê”ËˆaÊ•lanËŒat É¹iÊ”ËˆaËsatËŒu Ê”alÊ”ËˆarkaËnËŒi Ê”almËŒuÊƒtaÉ¹ËˆakatËŒi fiË siËwl.',
            "ar_spk_6",
            None,
            "ar",
        ],
        [
            'waËÊƒËŒintÌªu.n tËˆuÊ•iËdËŒu Ê”annnËˆaÃ°a.É¹ËŒa fËˆiË dÊ’ËˆuhuËdËŒi Ê”alwasËˆaËtÌªa.tËŒi liwËŒaqfi Ê”alqËˆitaËlËŒi fËˆiË Ê”assswdËˆaËni . fËˆiË siËËŒaËqi Ê”alÊ”ËˆazmatËŒi Ê”assswdaËnËˆiËatËŒi Ê”ËˆaidË¤ËŒa.n . qËˆaËlat wizËˆaËÉ¹atËŒu Ê”alÏ‡ËŒaËÉ¹idÊ’ËˆiËatËŒi Ê”alÊ”ËŒamÉ¹iËkËˆiËatËŒu Ê”Ëˆanna waËÊƒËŒintÌªu.n tËˆuÊ•iËdËŒu Ê”annnËˆaÃ°a.É¹ËŒa mËˆaÊ•a ÊƒËŒuÉ¹akËˆaËÊ”ihËŒaË Ê”alÊ”ËŒafaËÉ¹ËˆiqatËŒi waËlÊ•ËŒaÉ¹abËŒi . fËˆiË kaËŒifiËËŒati Ê”almËˆudË¤iËËŒi qËˆudumËŒan fËˆiË dÊ’ËˆuhuËdËŒi Ê”alwasËˆaËtÌªa.tËŒi fËˆiË Ê”asÌªsÌªËËˆi.É¹aËÊ•ËŒi fËˆiË Ê”assswdËˆaËni . watËˆaÊ”malËŒu fËˆiË tËˆaqdiËmËŒi tËŒausÌªËˆijjaËtËŒin biÄ§ËŒuluËlËŒi nihËˆaËjatËŒi Ê”alÊ”ËˆusbuËÊ•ËŒi . waÊ”ËˆaÊƒaËÉ¹ËŒat Ê”alwizËˆaËÉ¹atËŒu Ê”ËˆilaË Ê”ËˆannahËŒaË tËˆudÊ’É¹iË mËŒuÊƒaËwËˆaÉ¹aËtËŒin mËˆaÊ•a Ê”asssËŒuÊ•uËdËˆiËatËŒi walÊ”ËˆatÌªÉ¹aËfËŒi Ê”alÊ”ËŒifÉ¹iËqËˆiËatËŒi waËlÊ•ËŒaÉ¹abËŒi wËŒaÊƒuÉ¹ËˆakaËÊ”ËŒa Ê”aËÏ‡ËˆaÉ¹iËnËŒa . biÊƒËŒaÊ”ni Ê”atÌªËËˆa.É¹iËqËŒi lilmËŒudË¤iËËŒi qËˆudumËŒan fËˆiË Ä§Ëˆalli Ê”alÊ”ËˆazmatËŒi Ê”assswdaËnËˆiËatËŒi . waÊ”ËˆannahËŒaË taÊ•tËˆaqidËŒu Ê”ËˆannahËŒaË satËŒaÏ‡É¹udÊ’ËŒu bitËŒausÌªËˆijjaËtËŒin fËˆiË Ê”alÊ”aËˆiËaËmËŒi Ê”almuqbËˆilat. wËˆakaËnËŒa Ê”almasÊ”ËˆuËluËnËŒa Ê”alÊ”amÉ¹ËˆiËkiËwnËŒa wasssuÊ•ËˆuËdiËwnËŒa qËˆad Ä§ËˆaÃ°Ã°aÉ¹ËŒuË Ê”asssËˆabta mËˆin Ê”ËˆannahËŒum qËˆad jwqËˆifuËnËŒa dÊ’ËˆuhuËdËŒa Ê”alwasËˆaËtÌªa.tËŒi hËˆaÃ°ih. fËˆiË Ê”alwËˆaqti Ê”ËˆallaÃ°ËŒiË tËˆamma fiËhËŒi antËˆihaËkËŒu Ê”ËˆakÎ¸aÉ¹ËŒa mËˆin hËˆudnatËŒin biÊƒËŒaÊ”ni wËˆaqfi Ê”ËˆitÌªlaËqËŒi Ê”annnËˆaËÉ¹i mËˆin qËˆibalËŒi mËˆaË Ê”asmËˆauhËŒum biËŒaËlÊ”atÌªÉ¹ËŒaËfi Ê”almËŒutanaËzËˆiÊ•atËŒi fËˆiË Ê”assswdËˆaËni.',
            "ar_spk_7",
            None,
            "ar",
        ],
        [
            'Éªt tËˆÊŠk mËŒiË kwËˆaÉªt É lËˆÉ”Å‹ tËˆaÉªm tÉ™ dÉªvËˆÉ›lÉ™p É vËˆÉ”Éªs, Ã¦nd nËˆaÊŠ Ã°Ã¦t aÉª hÃ¦v ÉªÉ¾ aÉªm nËŒÉ‘Ët É¡ËŒoÊŠÉªÅ‹ tÉ™bi sËˆaÉªlÉ™nt.',
            "en_spk_1",
            None,
            "en",
        ],
        [
            'pÉ¹ËˆaÉªÉš tÉ™ noÊŠvËˆÉ›mbÉš twËˆÉ›nti twËˆÉ›nti Î¸É¹ËˆiË.',
            "en_spk_109",
            None,
            "en",
        ],
        [
            'bËˆÊŒr.eË bËˆÊŒr.eË dËˆeËÊƒoÌƒ meÌƒË ËˆÉ›Ësi cÊ°ËˆoËÊˆi cÊ°ËˆoËÊˆi bËˆaËteÌƒË hËˆoËti É¾ËˆÊŒhÉ™tËŒi hÉ›Ë',
            "hi_spk_1",
            None,
            "hi",
        ],
        [
            'nËˆaËÉ–inoËÊˆ pÉ¾ËˆÉdibËŒÉddÊ°É™dÉ uÉ³É–ËˆaËjaËl mËˆaËtÉ¾É™meË vËˆiÉ¡É™sËŒÉnÉ™m uÉ³É–ËˆaËÉ¡uÉ¡ËŒÉjuÉ­É­ËŒuËvennÉ¨ mËˆukÊ°jÉ™mËŒÉntÉ¾i pËˆiÉ³É™rËŒaËji vËˆiÉŸÉ™jÉ™n',
            "ml_spk_1",
            None,
            "ml",
        ],
        [
            'cËˆesÉ¨ lËˆoËÉ¡É™É¡ËŒÉpËÉ¨ kiÉ¾ËˆiËÉ–É™m lËˆoËÉ¡É onnËˆaËm nËˆÉmbÉ™É¾ tËˆaËÉ¾É™vum mËˆun lËˆoËÉ¡É™cËŒaËmpjÉ™nËŒumaËjÉ nËˆoËÉ¾vejËŒuÉ–e mËˆaËÉ¡nÉ™sÉ¨ kËˆaËÉ­sÉ™nÉ¨',
            "ml_spk_5",
            None,
            "ml",
        ],
        [
            'vËˆiÉ•vÉ™nËŒaËtÊ°É™n ËˆaËnÉ™ndËŒinuÉ•ËŒeËÊ‚É™m cËˆesÉ¨ lËˆoËÉ¡É™É¡ËŒÉpËÉ¨ pÊ°ËˆaÉªnÉ™lËŒÉlilËŒetËunnÉ ËˆaËdjÉ ËˆintjÉ™n tËˆaËÉ¾É™mËŒennÉ nËˆeËÊˆËÉ™m nËˆeËÉ¾É™tËeË tËˆÉnne pÉ¾ÉÉ¡nËˆaËnÉ™ndÉ svËˆÉndÉ™mËŒaËkËijËŒiÉ¾unnu.',
            "ml_spk_19",
            None,
            "ml",
        ],
    ],
    # Enable the weighted sampler
    use_weighted_sampler=True,
    # Ensures that all speakers are seen in the training batch equally no matter how many samples each speaker has
    weighted_sampler_attrs={"speaker_name": 1.0},
    weighted_sampler_multipliers={},
    # It defines the Speaker Consistency Loss (SCL) Î± to 9 like the paper
    speaker_encoder_loss_alpha=9.0,
    save_all_best=True
)

# Load all the datasets samples and split traning and evaluation sets
for dataset in config.datasets:
    print(dataset)

train_samples, eval_samples = load_tts_samples(
    config.datasets,
    eval_split=True,
    eval_split_max_size=config.eval_split_max_size,
    eval_split_size=config.eval_split_size,
)

# Init the model
model = Vits.init_from_config(config)

# Init the trainer and ğŸš€
trainer = Trainer(
    TrainerArgs(restore_path=RESTORE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),
    config,
    output_path=EXPMT_PATH,
    model=model,
    train_samples=train_samples,
    eval_samples=eval_samples,
)
trainer.fit()
